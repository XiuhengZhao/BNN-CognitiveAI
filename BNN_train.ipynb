{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset,random_split,SubsetRandomSampler, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_cont = torch.tensor(self.dataframe.iloc[idx]['X_cont'], dtype=torch.float32)\n",
    "\n",
    "        other_cols = self.dataframe.drop(columns=['label', 'X_cont','Unit1'])\n",
    "        x_other = torch.tensor(other_cols.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "        return x_cont, x_other, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchbnn' has no attribute 'BayesConv1d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xq/37sksygx4qv7jb_hjf_vww6r0000gn/T/ipykernel_23815/654593225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBayesConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchbnn' has no attribute 'BayesConv1d'"
     ]
    }
   ],
   "source": [
    "class BayesianCNN(nn.Module):\n",
    "    def __init__(self, num_channels, output_size):\n",
    "        super(BayesianCNN, self).__init__()\n",
    "        # Convolutional layers for processing time series data\n",
    "        self.conv1 = nn.Conv1d(num_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers to process sequential data after convolution layers\n",
    "        self.fc1 = nn.Linear(64, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "        # MLP layers for processing static data\n",
    "        self.fc_static1 = nn.Linear(output_size, 64)\n",
    "        self.fc_static2 = nn.Linear(64, 64)\n",
    "        self.fc_static3 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Final layer to combine features from time-series and static data processing\n",
    "        self.fc_final = nn.Linear(20, 1)\n",
    "\n",
    "    # Forward pass through the network\n",
    "    def forward(self, x_dynamic, x_static):\n",
    "        # Dynamic part processing using convolutional layers\n",
    "        x = self.conv1(x_dynamic)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=-1)  # Global average pooling to reduce dimensionality\n",
    "        \n",
    "        # Passing through the first set of fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        # Static part processing using MLP layers\n",
    "        y = torch.relu(self.fc_static1(x_static))\n",
    "        y = torch.relu(self.fc_static2(y))\n",
    "        y = torch.relu(self.fc_static3(y))\n",
    "\n",
    "        # Combining the outputs from both dynamic and static processing paths\n",
    "        z = torch.cat((x, y), dim=1)\n",
    "        z = torch.sigmoid(self.fc_final(z))  # Sigmoid activation for binary classification output\n",
    "        return z\n",
    "    def model(self, x_dynamic, x_static, y=None):\n",
    "        # Define prior distributions for all neural network weights\n",
    "        priors = {\n",
    "            'conv1.weight': dist.Normal(0, 1).expand([16, x_dynamic.size(1), 3]).to_event(3),\n",
    "            'conv2.weight': dist.Normal(0, 1).expand([32, 16, 3]).to_event(3),\n",
    "            'conv3.weight': dist.Normal(0, 1).expand([64, 32, 3]).to_event(3),\n",
    "            'fc1.weight': dist.Normal(0, 1).expand([50, 64]).to_event(2),\n",
    "            'fc2.weight': dist.Normal(0, 1).expand([10, 50]).to_event(2),\n",
    "            'fc_static1.weight': dist.Normal(0, 1).expand([64, x_static.size(1)]).to_event(2),\n",
    "            'fc_static2.weight': dist.Normal(0, 1).expand([64, 64]).to_event(2),\n",
    "            'fc_static3.weight': dist.Normal(0, 1).expand([10, 64]).to_event(2),\n",
    "            'fc_final.weight': dist.Normal(0, 1).expand([1, 20]).to_event(2),\n",
    "        }\n",
    "        lifted_module = pyro.random_module(\"module\", self, priors)  # Lift module parameters to random variables\n",
    "        lifted_reg_model = lifted_module()\n",
    "\n",
    "        # Condition on the observed data\n",
    "        with pyro.plate(\"data\", x_dynamic.size(0)):\n",
    "            prediction = lifted_reg_model(x_dynamic, x_static)\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(prediction).to_event(1), obs=y)\n",
    "\n",
    "    # Defining the guide function for variational inference\n",
    "    def guide(self, x_dynamic, x_static, y=None):\n",
    "        # Define variational distributions for the parameters (learnable)\n",
    "        softplus = torch.nn.Softplus()\n",
    "        priors = {\n",
    "            'conv1.weight': dist.Normal(torch.randn([16, x_dynamic.size(1), 3]), softplus(torch.randn([16, x_dynamic.size(1), 3]))).to_event(3),\n",
    "            'conv2.weight': dist.Normal(torch.randn([32, 16, 3]), softplus(torch.randn([32, 16, 3]))).to_event(3),\n",
    "            'conv3.weight': dist.Normal(torch.randn([64, 32, 3]), softplus(torch.randn([64, 32, 3]))).to_event(3),\n",
    "            'fc1.weight': dist.Normal(torch.randn([50, 64]), softplus(torch.randn([50, 64]))).to_event(2),\n",
    "            'fc2.weight': dist.Normal(torch.randn([10, 50]), softplus(torch.randn([10, 50]))).to_event(2),\n",
    "            'fc_static1.weight': dist.Normal(torch.randn([64, x_static.size(1)]), softplus(torch.randn([64, x_static.size(1)]))).to_event(2),\n",
    "            'fc_static2.weight': dist.Normal(torch.randn([64, 64]), softplus(torch.randn([64, 64]))).to_event(2),\n",
    "            'fc_static3.weight': dist.Normal(torch.randn([10, 64]), softplus(torch.randn([10, 64]))).to_event(2),\n",
    "            'fc_final.weight': dist.Normal(torch.randn([1, 20]), softplus(torch.randn([1, 20]))).to_event(2),\n",
    "        }\n",
    "        lifted_module = pyro.random_module(\"module\", self, priors)  # Lift module parameters to random variables\n",
    "        return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, guide, train_loader, num_epochs=5):\n",
    "    optim = Adam({\"lr\": 0.01})\n",
    "    svi = SVI(model.model, model.guide, optim, loss=Trace_ELBO())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x_dynamic, x_static, y in train_loader:\n",
    "            loss = svi.step(x_dynamic, x_static, y)\n",
    "            total_loss += loss\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
